{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Union, Dict\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Column\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 14:38:55 WARN Utils: Your hostname, Emilianos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.130 instead (on interface en0)\n",
      "23/07/21 14:38:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/21 14:38:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/21 14:38:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# master configuration to use only 4 CPU cores\n",
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
    "\n",
    "# basic configuration to use only a reasonable number of partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partition\", 4)\n",
    "\n",
    "# configuration to work in UTC\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df: DataFrame, columns_to_drop: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Drop specified columns from a PySpark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        columns_to_drop (list): List of column names to drop.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with specified columns dropped.\n",
    "    \"\"\"\n",
    "    return df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", 28, \"F\", \"Engineer\"),\n",
    "    (\"Bob\", 32, \"M\", \"Doctor\"),\n",
    "    (\"Charlie\", 45, \"M\", \"Lawyer\"),\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Gender\", \"Profession\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before dropping columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender|Profession|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 28|     F|  Engineer|\n",
      "|    Bob| 32|     M|    Doctor|\n",
      "|Charlie| 45|     M|    Lawyer|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame before dropping columns\n",
    "print(\"DataFrame before dropping columns:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after dropping columns:\n",
      "+-------+------+\n",
      "|   Name|Gender|\n",
      "+-------+------+\n",
      "|  Alice|     F|\n",
      "|    Bob|     M|\n",
      "|Charlie|     M|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\"Age\", \"Profession\"]\n",
    "\n",
    "# Drop the specified columns using the function\n",
    "df_dropped = drop_columns(df, columns_to_drop)\n",
    "\n",
    "# Show the DataFrame after dropping columns\n",
    "print(\"DataFrame after dropping columns:\")\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column(df: DataFrame, config: dict) -> DataFrame:\n",
    "    new_column = config[\"column\"]\n",
    "    value_column = config[\"value_column\"]\n",
    "    return df.withColumn(new_column, F.lit(value_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
