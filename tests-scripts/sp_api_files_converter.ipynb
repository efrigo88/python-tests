{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422ba920-e9ec-4e18-a402-9e75cbd1fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "aws_session = boto3.session.Session(profile_name=\"data-maintainer-prod\")\n",
    "s3_client = aws_session.client(\"s3\")\n",
    "s3 = aws_session.resource(\"s3\")\n",
    "\n",
    "landing_bucket = s3.Bucket(\"f14-datalake-landing-prod\")\n",
    "input_bucket = \"f14-datalake-landing-prod\"\n",
    "output_bucket = \"f14-datalake-raw-prod\"\n",
    "base_prefix = \"amazon_sp_api/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9288fcb-802e-4ce3-961d-d41558652517",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5bd1faa-f34a-4f94-b32a-aff5d5f9ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import progressbar\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "\n",
    "def read_from_s3(s3_client, bucket_name, file_prefix):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_prefix)\n",
    "    body = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "    return body\n",
    "\n",
    "\n",
    "def upload_to_s3(s3_client, input_file_path, output_bucket, output_prefix):\n",
    "    with open(input_file_path, \"rb\") as f:\n",
    "        object_data = f.read()\n",
    "        s3_client.put_object(Body=object_data, Bucket=output_bucket, Key=output_prefix)\n",
    "\n",
    "\n",
    "def replace_with_raw_extension(input_file_name):\n",
    "    return input_file_name[: input_file_name.find(\".\")] + \".jsonl\"\n",
    "\n",
    "\n",
    "def get_files_to_proccess(valid_prefix):\n",
    "    files_to_process = []\n",
    "    for landing_bucket_object in landing_bucket.objects.all():\n",
    "        object_last_update = landing_bucket_object.last_modified\n",
    "        bucket_name = landing_bucket_object.bucket_name\n",
    "        object_key = landing_bucket_object.key\n",
    "\n",
    "        if valid_prefix in object_key and (\n",
    "            \".csv\" in object_key or \".tsv\" in object_key or \".jsonl\" in object_key\n",
    "        ):\n",
    "            files_to_process.append([bucket_name, object_key, object_last_update])\n",
    "\n",
    "    return files_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747d6e1-737b-48c2-84df-32bb447959c1",
   "metadata": {},
   "source": [
    "## Scraper custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11e4d9-ae94-421f-9d43-39c9e9c680e4",
   "metadata": {},
   "source": [
    "### Parse input prefix to get output prefix\n",
    "Input prefix example: s3://f14-datalake-landing-prod/amazon_sp_api/GET_FBA_FULFILLMENT_CURRENT_INVENTORY_DATA/p_creation_dt=2021-10-27/p_brand_id=BARVIVO/p_region_id=EU/768408018927.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de5b22b-3d5a-4b26-b2a8-a135927005d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_destination_prefix(file_prefix, last_modified_dt=datetime.now()):\n",
    "    file_prefix = file_prefix.split(\"/\")\n",
    "    file_name = file_prefix[-1]\n",
    "    creation_dt = file_prefix[2].replace(\"p_creation_dt=\", \"\")\n",
    "    brand = file_prefix[3].replace(\"p_brand_id=\", \"\")\n",
    "    region = file_prefix[4].replace(\"p_region_id=\", \"\")\n",
    "    entity_name = file_prefix[1].lower()\n",
    "\n",
    "    destination_prefix = (\n",
    "        f\"amazon_sp_api/{entity_name}/p_creation_dt={creation_dt}/{file_name}\"\n",
    "    )\n",
    "    return replace_with_raw_extension(destination_prefix), brand, region\n",
    "\n",
    "\n",
    "def fix_cols_names(df):\n",
    "    dest_cols = []\n",
    "    for col in df.columns:\n",
    "        new_name = (\n",
    "            col.lower()\n",
    "            .replace(\"-\", \"_\")\n",
    "            .replace(\" \", \"_\")\n",
    "            .replace(\"___\", \"_\")\n",
    "            .replace(\"(\", \"\")\n",
    "            .replace(\")\", \"\")\n",
    "        )\n",
    "        dest_cols.append(new_name)\n",
    "        df[new_name] = df[col]\n",
    "\n",
    "    return df[dest_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a497d-4628-489c-b497-ae0ab8fb8d8a",
   "metadata": {},
   "source": [
    "### Convert file content to JSON list and save in temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f277ae1-99a7-4bd7-93b8-c9b3842d683d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_and_upload_parquet(file_content, brand_id, region_id, output_prefix):\n",
    "    df = pd.read_csv(\n",
    "        io.StringIO(file_content),\n",
    "        quotechar='\"',\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "        delim_whitespace=False,\n",
    "    )\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    df[\"brand_id\"] = brand_id.upper()\n",
    "    df[\"region_id\"] = region_id.upper()\n",
    "    df[\"aud_process_ts\"] = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "    df = fix_cols_names(df)\n",
    "\n",
    "    temp_file_path = f\"temp/{output_prefix}\"\n",
    "    destination_folder = temp_file_path[: temp_file_path.rfind(\"/\")]\n",
    "\n",
    "    if not os.path.exists(destination_folder):\n",
    "        pathlib.Path(destination_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df.to_json(temp_file_path, orient=\"records\", lines=True)\n",
    "    return temp_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565e622-17d3-49f8-b9cb-9e1c7fdbed0e",
   "metadata": {},
   "source": [
    "### Process File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe09218-d963-4605-b3ec-972d71a1bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(s3_client, file_to_process):\n",
    "    last_modified_dt = file_to_process[2]\n",
    "    input_bucket = file_to_process[0]\n",
    "    input_prefix = file_to_process[1]\n",
    "\n",
    "    file_content = read_from_s3(s3_client, input_bucket, input_prefix)\n",
    "    output_prefix, brand_id, region_id = get_destination_prefix(input_prefix)\n",
    "    temp_file_path = convert_and_upload_parquet(\n",
    "        file_content, brand_id, region_id, output_prefix\n",
    "    )\n",
    "    if temp_file_path:\n",
    "        upload_to_s3(s3_client, temp_file_path, output_bucket, output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369bd7cd-cf5f-4824-b760-503e626cb8f1",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43333c7c-acb7-41ed-96c7-fac776090710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import progressbar\n",
    "import io\n",
    "\n",
    "files_to_process_all = get_files_to_proccess(base_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c77c8a-7659-4ace-ad57-403c9a3e0717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "previous_day_str = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "current_day_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "files_to_process = [\n",
    "    file_prefix\n",
    "    for file_prefix in files_to_process_all\n",
    "    if previous_day_str in file_prefix[1] or current_day_str in file_prefix[1]\n",
    "]\n",
    "len(files_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1351e-8896-435f-85d7-c7edf7152265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13% (43 of 329) |###                    | Elapsed Time: 0:01:54 ETA:   2:25:45"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with progressbar.ProgressBar(max_value=len(files_to_process) + 1) as bar:\n",
    "    for file_to_process in files_to_process:\n",
    "        process_file(s3_client, file_to_process)\n",
    "        i = i + 1\n",
    "        bar.update(i)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00b39f-24aa-45fc-a9f8-da0a2c31ecf4",
   "metadata": {},
   "source": [
    "## Functions Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fb790-8d04-41d8-a83d-d31b83a7cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_prefix = \"amazon_sp_api/GET_FBA_FULFILLMENT_CURRENT_INVENTORY_DATA/p_creation_dt=2021-10-27/p_brand_id=BARVIVO/p_region_id=EU/768408018927.tsv\"\n",
    "get_destination_prefix(test_input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa87383-c934-4fbc-a89f-f6c59c7940ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_process = get_files_to_proccess(base_prefix)\n",
    "files_to_process[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f771988-e31f-4f2f-8e4e-0045a7def9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b24c87-5909-4152-aa5f-ce7f24bd978f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
