{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 10:58:35 WARN Utils: Your hostname, emif-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.171 instead (on interface en0)\n",
      "22/06/09 10:58:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/09 10:58:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/09 10:58:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"SparkSQLExampleApp\").master(\"local[4]\").getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partition\", 4)\n",
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"/Users/emif/Documents/spark-lab/datasets/departuredelays.csv\"\n",
    "\n",
    "schema = \"\"\"date STRING, \n",
    "            delay INT, \n",
    "            distance INT,\n",
    "            origin STRING,\n",
    "            destination STRING\n",
    "\"\"\"\n",
    "\n",
    "# Read file\n",
    "df = spark.read.format(\"csv\").schema(schema).option(\"header\", \"true\").load(csv_file)\n",
    "# create temporal view\n",
    "# df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "CREATE TABLE us_delay_flights_ext_tbl(date STRING, \n",
    "                                      delay INT,\n",
    "                                      distance INT,\n",
    "                                      origin STRING, \n",
    "                                      destination STRING)\n",
    "USING csv OPTIONS (PATH '/Users/emif/Documents/spark-lab/datasets/departuredelays.csv')\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------------+-----------+\n",
      "|namespace     |tableName                   |isTemporary|\n",
      "+--------------+----------------------------+-----------+\n",
      "|learn_spark_db|managed_us_delay_flights_tbl|false      |\n",
      "+--------------+----------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/Users/emif/Documents/spark-lab/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/Users/emif/Documents/spark-lab/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"Drop DATABASE if exists learn_spark_db cascade\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fe32d98a1ee141d9be34c10c16fbd81de181f41d0001c931301c6474f8fba72"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
