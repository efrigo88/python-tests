{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Union, Dict\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Column\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_delete_condition(df: DataFrame, col_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SQL-like condition string to use in a DELETE statement\n",
    "    based on distinct values in a DataFrame column.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        col_name (str): The name of the column in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        str: A condition string in the format \"{col_name} IN (value1, value2, ...)\".\n",
    "    \"\"\"\n",
    "    distinct_values = df.select(col_name).distinct().collect()\n",
    "    condition = (\n",
    "        f\"{col_name} IN (\"\n",
    "        + \", \".join(f\"'{row[col_name]}'\" for row in distinct_values)\n",
    "        + \")\"\n",
    "    )\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_delete_condition(df: DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a combined SQL-like condition string to use in a DELETE statement\n",
    "    based on distinct values in multiple columns of a DataFrame.\n",
    "\n",
    "    If the DataFrame contains only one column, a simple delete condition for that column is returned.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        str: A combined condition string using AND for multiple columns, or a simple condition string for a single column.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(df.columns) == 1:\n",
    "        return get_simple_delete_condition(df, df.columns[0])\n",
    "    else:\n",
    "        where_condition = get_simple_delete_condition(df, df.columns[0])\n",
    "        i = 1\n",
    "        while i < len(df.columns):\n",
    "            where_condition += \" AND \" + get_simple_delete_condition(df, df.columns[i])\n",
    "            i += 1\n",
    "\n",
    "        return where_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeDeleteCondition\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  col1|col2|\n",
      "+------+----+\n",
      "|value1|   A|\n",
      "|value1|   A|\n",
      "|value1|   B|\n",
      "|value2|   D|\n",
      "|value2|   E|\n",
      "|value3|   Z|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"value1\", \"A\"),\n",
    "    (\"value1\", \"A\"),\n",
    "    (\"value1\", \"B\"),\n",
    "    (\"value2\", \"D\"),\n",
    "    (\"value2\", \"E\"),\n",
    "    (\"value3\", \"Z\"),\n",
    "]\n",
    "\n",
    "# Assuming you have a DataFrame with two columns named 'col1' and 'col2'\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1 IN ('value1', 'value2', 'value3') AND col2 IN ('A', 'B', 'D', 'E', 'Z')\n"
     ]
    }
   ],
   "source": [
    "# Generate the WHERE condition\n",
    "where_condition = generate_delete_condition(df)\n",
    "\n",
    "# Output the WHERE condition\n",
    "print(where_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   col|\n",
      "+------+\n",
      "|value1|\n",
      "|value1|\n",
      "|value2|\n",
      "|value3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data_simple = [(\"value1\",), (\"value1\",), (\"value2\",), (\"value3\",)]\n",
    "\n",
    "# Assuming you have a DataFrame with one column named 'col'\n",
    "df_simple = spark.createDataFrame(data_simple, [\"col\"])\n",
    "\n",
    "df_simple.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col IN ('value1', 'value2', 'value3')\n"
     ]
    }
   ],
   "source": [
    "where_condition_simple = generate_delete_condition(df_simple)\n",
    "\n",
    "print(where_condition_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  1|     A|\n",
      "|  2|     B|\n",
      "|  3|     C|\n",
      "|  1|     A|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (1, \"A\")]\n",
    "columns = [\"id\", \"letter\"]\n",
    "\n",
    "df_test = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id IN ('1', '2', '3') AND letter IN ('A', 'B', 'C')\n"
     ]
    }
   ],
   "source": [
    "where_condition_test = generate_delete_condition(df_test)\n",
    "\n",
    "print(where_condition_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Convert the date strings to DateType\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data:\n\u001b[0;32m---> 18\u001b[0m     d[\u001b[39m\"\u001b[39m\u001b[39mposted_dt\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mto_date(d[\u001b[39m\"\u001b[39;49m\u001b[39mposted_dt\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     20\u001b[0m initial_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(data, data_schema)\n\u001b[1;32m     22\u001b[0m initial_df\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "data_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"id\", T.IntegerType(), True),\n",
    "        T.StructField(\"posted_dt\", T.DateType(), True),\n",
    "        T.StructField(\"name\", T.StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = (\n",
    "    [\n",
    "        {\"id\": 1, \"posted_dt\": \"2023-08-01\", \"name\": \"John1\"},\n",
    "        {\"id\": 1, \"posted_dt\": \"2023-08-01\", \"name\": \"John2\"},\n",
    "        {\"id\": 2, \"posted_dt\": \"2023-08-02\", \"name\": \"Mary\"},\n",
    "        {\"id\": 3, \"posted_dt\": \"2023-08-01\", \"name\": \"Teresa\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Convert the date strings to DateType\n",
    "for d in data:\n",
    "    d[\"posted_dt\"] = F.to_date(d[\"posted_dt\"])\n",
    "\n",
    "initial_df = spark.createDataFrame(data, data_schema)\n",
    "\n",
    "initial_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id| posted_dt|  name|\n",
      "+---+----------+------+\n",
      "|  1|2023-08-01| John1|\n",
      "|  1|2023-08-01| John2|\n",
      "|  2|2023-08-02|  Mary|\n",
      "|  3|2023-08-01|Teresa|\n",
      "+---+----------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- posted_dt: date (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"id\", T.IntegerType(), True),\n",
    "        T.StructField(\"posted_dt\", T.StringType(), True),\n",
    "        T.StructField(\"name\", T.StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "initial_data = [\n",
    "    {\"id\": 1, \"posted_dt\": \"2023-08-01\", \"name\": \"John1\"},\n",
    "    {\"id\": 1, \"posted_dt\": \"2023-08-01\", \"name\": \"John2\"},\n",
    "    {\"id\": 2, \"posted_dt\": \"2023-08-02\", \"name\": \"Mary\"},\n",
    "    {\"id\": 3, \"posted_dt\": \"2023-08-01\", \"name\": \"Teresa\"},\n",
    "]\n",
    "\n",
    "\n",
    "initial_df = spark.createDataFrame(initial_data, data_schema)\n",
    "\n",
    "initial_df = initial_df.withColumn(\n",
    "    \"posted_dt\", F.to_date(\"posted_dt\", format=\"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "initial_df.show()\n",
    "initial_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
