{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Union\n",
    "from collections import OrderedDict\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import DataFrame, SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "domain = os.getenv(\"COMPANY_DOMAIN\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "encryption_key = os.getenv(\"ENCRYPTION_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BambooHR API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPLOYEES_REPORT_PAYLOAD = {\n",
    "    \"title\": \"custom-employees\",\n",
    "    \"fields\": [\n",
    "        \"displayName\",\n",
    "        \"firstName\",\n",
    "        \"lastName\",\n",
    "        \"preferredName\",\n",
    "        \"dateOfBirth\",\n",
    "        \"maritalStatus\",\n",
    "        \"ssn\",\n",
    "        \"gender\",\n",
    "        \"pronouns\",\n",
    "        \"country\",\n",
    "        \"city\",\n",
    "        \"zipcode\",\n",
    "        \"address1\",\n",
    "        \"address2\",\n",
    "        \"employee_access\",\n",
    "        \"customShirtsize\",\n",
    "        \"customHobbies\",\n",
    "        \"allergies\",\n",
    "        \"dietaryRestrictions\",\n",
    "        \"jobTitle\",\n",
    "        \"hireDate\",\n",
    "        \"originalHireDate\",\n",
    "        \"employeeStatusDate\",\n",
    "        \"employmentStatus\",\n",
    "        \"employmentHistoryStatus\",  # FTE\n",
    "        \"terminationDate\",\n",
    "        \"location\",\n",
    "        \"workPhone\",\n",
    "        \"mobilePhone\",\n",
    "        \"workEmail\",\n",
    "        \"department\",\n",
    "        \"division\",\n",
    "        \"workPhoneExtension\",\n",
    "        \"supervisor\",\n",
    "        \"supervisorEid\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BambooHRClient:\n",
    "    \"\"\"A client for the BambooHR API.\n",
    "    This client handles authentication, session management, and API requests\n",
    "    to the BambooHR API or a placeholder API for demonstration purposes.\n",
    "    \"\"\"\n",
    "\n",
    "    ENDPOINTS = {\n",
    "        \"company_information\": \"/company_information\",\n",
    "        # Headcount\n",
    "        \"employees\": \"/reports/custom\",\n",
    "        \"employees_changed\": \"/employees/changed\",\n",
    "        \"account_info_fields\": \"/meta/fields\",\n",
    "        \"account_info_tab_fields\": \"/meta/tables\",\n",
    "        \"account_list_fields\": \"/meta/lists\",\n",
    "        \"account_list_users\": \"/meta/users\",\n",
    "        # Time Off\n",
    "        \"time_off_types\": \"/meta/time_off/types\",\n",
    "        \"time_off_policies\": \"/meta/time_off/policies\",\n",
    "        \"time_off_requests\": \"/time_off/requests\",\n",
    "        \"time_off_whos_out\": \"/time_off/whos_out\",\n",
    "        # Hiring\n",
    "        \"applicant_tracking_jobs\": \"/applicant_tracking/jobs\",\n",
    "        \"applicant_tracking_applications\": \"/applicant_tracking/applications\",\n",
    "        \"applicant_application_details\": \"/applicant_tracking/applications/{appId}\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, company_domain: str, api_key: str, days_offset: int = 1):\n",
    "        \"\"\"Initialize the BambooHRClient with an API key and a dynamically set domain.\"\"\"\n",
    "        self.domain = company_domain\n",
    "        self.base_url = f\"https://api.bamboohr.com/api/gateway.php/{self.domain}/v1\"\n",
    "        self.api_key = api_key\n",
    "        self.days_offset = days_offset\n",
    "        self.employees_report_paylod = EMPLOYEES_REPORT_PAYLOAD\n",
    "        self.session = self._create_session()\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": self._encode_auth_header(api_key),\n",
    "        }\n",
    "\n",
    "    def _encode_auth_header(self, api_key: str) -> str:\n",
    "        \"\"\"Encode the API key in Base64 for the Authorization header.\"\"\"\n",
    "        auth_string = f\"{api_key}:x\"\n",
    "        encoded_bytes = base64.b64encode(auth_string.encode(\"utf-8\"))\n",
    "        return f\"Basic {encoded_bytes.decode('utf-8')}\"\n",
    "\n",
    "    def _build_query_string(self, params: dict) -> str:\n",
    "        \"\"\"Helper method to construct a query string from a dictionary.\"\"\"\n",
    "        return \"&\".join(f\"{k}={v}\" for k, v in params.items())\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create a session with a retry strategy for handling transient errors.\"\"\"\n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=2,\n",
    "            status_forcelist=[429, 503],\n",
    "            allowed_methods=[\"GET\", \"POST\"],\n",
    "        )\n",
    "\n",
    "        # Create an adapter with the retry strategy\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "        # Create a session and mount the adapter\n",
    "        session = requests.Session()\n",
    "        session.mount(\"https://\", adapter)\n",
    "\n",
    "        return session\n",
    "\n",
    "    def get(\n",
    "        self, endpoint_key: str = None, endpoint_path: str = None, params: dict = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch data from a specified endpoint using the endpoint key or a direct endpoint path.\n",
    "\n",
    "        Args:\n",
    "            endpoint_key (str, optional): The key for the desired endpoint (e.g., \"posts\"). Defaults to None.\n",
    "            endpoint_path (str, optional): The direct path for the endpoint (e.g., \"/employees/123\"). Defaults to None.\n",
    "            params (dict, optional): Query parameters to include in the request.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If neither `endpoint_key` nor `endpoint_path` is provided.\n",
    "            requests.exceptions.RequestException: If the request fails.\n",
    "        \"\"\"\n",
    "        if endpoint_key:\n",
    "            endpoint = self.ENDPOINTS.get(endpoint_key)\n",
    "            if not endpoint:\n",
    "                raise ValueError(f\"Invalid endpoint key: {endpoint_key}\")\n",
    "        elif endpoint_path:\n",
    "            endpoint = endpoint_path\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"You must provide either an endpoint_key or an endpoint_path.\"\n",
    "            )\n",
    "\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        try:\n",
    "            response = self.session.get(url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with status {response.status_code}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def get_employees_changed(self, change_type: str = None) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch the list of employees changed since a given number of days ago.\n",
    "\n",
    "        Args:\n",
    "            change_type (str, optional): Type of change to filter for (\"inserted\", \"updated\", \"deleted\").\n",
    "                If not provided, all change types will be included.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API, containing the list of employees\n",
    "                who have changed since the specified timestamp.\n",
    "        \"\"\"\n",
    "        since = datetime.now(timezone.utc) - timedelta(days=self.days_offset)\n",
    "        since = since.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        params = {\"since\": since}\n",
    "        if change_type:\n",
    "            params[\"type\"] = change_type\n",
    "\n",
    "        return self.get(endpoint_key=\"employees_changed\", params=params)\n",
    "\n",
    "    def get_time_off_request(self) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch time-off requests within a specified date range.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API, containing the list of time-off\n",
    "                requests within the specified date range.\n",
    "        \"\"\"\n",
    "        start_dt = datetime.now(timezone.utc) - timedelta(days=self.days_offset)\n",
    "        end_dt = datetime.now(timezone.utc)\n",
    "        start_dt = start_dt.strftime(\"%Y-%m-%d\")\n",
    "        end_dt = end_dt.strftime(\"%Y-%m-%d\")\n",
    "        params = {\n",
    "            \"start\": start_dt,\n",
    "            \"end\": end_dt,\n",
    "        }\n",
    "\n",
    "        return self.get(endpoint_key=\"time_off_requests\", params=params)\n",
    "\n",
    "    def get_appl_trk_apps(self) -> dict:\n",
    "        \"\"\"\n",
    "        Fetches applicant tracking applications updated since a specified number of days ago.\n",
    "\n",
    "        This method generates the `newSince` query parameter using the current UTC time\n",
    "        minus the specified number of days. It then sends a GET request to the\n",
    "        `applicant_tracking_applications` endpoint with the constructed parameter.\n",
    "\n",
    "        Returns:\n",
    "            dict: The API response containing applicant tracking applications.\n",
    "        \"\"\"\n",
    "        new_since = datetime.now(timezone.utc) - timedelta(days=self.days_offset)\n",
    "        new_since = new_since.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        params = {\"newSince\": new_since}\n",
    "\n",
    "        return self.get(endpoint_key=\"applicant_tracking_applications\", params=params)\n",
    "\n",
    "    def get_application_details(self) -> list:\n",
    "        \"\"\"\n",
    "        Fetches detailed information for each application in the applicant tracking system.\n",
    "\n",
    "        This method first retrieves a list of applications using the `get_appl_trk_apps` method,\n",
    "        extracts their IDs, and then fetches detailed information for each application by calling\n",
    "        the BambooHR API endpoint `/applicant_tracking/applications/{applicationId}`.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, where each dictionary contains detailed data for a single application.\n",
    "\n",
    "        Notes:\n",
    "            - The method uses the `get_appl_trk_apps` method to fetch the initial list of applications.\n",
    "            - The `get` method is used with `endpoint_path` to fetch details for each `applicationId`.\n",
    "\n",
    "        Error Handling:\n",
    "            - If an exception occurs during the detailed fetch for a specific application,\n",
    "            the method logs the error and continues processing the remaining application IDs.\n",
    "        \"\"\"\n",
    "        # Fetch the initial applications report and get the application IDs\n",
    "        applications_report = self.get_appl_trk_apps()\n",
    "        application_ids = [\n",
    "            app[\"id\"] for app in applications_report.get(\"applications\", [])\n",
    "        ]\n",
    "        # Fetch detailed data for each application\n",
    "        detailed_applications = []\n",
    "        for app_id in application_ids:\n",
    "            try:\n",
    "                details = self.get(\n",
    "                    endpoint_path=f\"/applicant_tracking/applications/{app_id}\"\n",
    "                )\n",
    "                detailed_applications.append(details)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch details for application ID {app_id}: {e}\")\n",
    "\n",
    "        return detailed_applications\n",
    "\n",
    "    def post(\n",
    "        self, endpoint_key: str, data: dict = None, query_params: dict = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Send data to a specified endpoint using the POST method.\n",
    "\n",
    "        Args:\n",
    "            endpoint_key (str): The key for the desired endpoint.\n",
    "            data (dict, optional): The JSON payload to send in the POST request.\n",
    "            query_params (dict, optional): Query parameters to include in the URL.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an invalid endpoint key is provided.\n",
    "            requests.exceptions.RequestException: If the request fails.\n",
    "        \"\"\"\n",
    "        endpoint = self.ENDPOINTS.get(endpoint_key)\n",
    "        if not endpoint:\n",
    "            raise ValueError(f\"Invalid endpoint key: {endpoint_key}\")\n",
    "\n",
    "        if query_params:\n",
    "            query_string = self._build_query_string(query_params)\n",
    "            url = f\"{self.base_url}{endpoint}?{query_string}\"\n",
    "        else:\n",
    "            url = f\"{self.base_url}{endpoint}\"\n",
    "\n",
    "        try:\n",
    "            response = self.session.post(url, headers=self.headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with status {response.status_code}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_employees_report(self) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a custom report of employees and all their relevant data.\n",
    "\n",
    "        Returns:\n",
    "            dict: The formatted report with each employee's data as a separate row.\n",
    "        \"\"\"\n",
    "        payload = EMPLOYEES_REPORT_PAYLOAD\n",
    "        q_params = {\"format\": \"json\", \"onlyCurrent\": \"true\"}\n",
    "        response = self.post(\"employees\", data=payload, query_params=q_params)\n",
    "\n",
    "        # Get only employees from response\n",
    "        employees = response.get(\"employees\", [])\n",
    "\n",
    "        # Return formatted employee rows with consistent fields\n",
    "        # Use OrderedDict to preserve key order for each employee\n",
    "        return [\n",
    "            OrderedDict((key, employee.get(key)) for key in employee)\n",
    "            for employee in employees\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client = BambooHRClient(api_key=api_key, days_offset=30)\n",
    "\n",
    "# Create a data directory to store the JSON outputs\n",
    "os.makedirs(\"data/api_response\", exist_ok=True)\n",
    "\n",
    "\n",
    "def save_json_data(data: dict, filename: str) -> None:\n",
    "    \"\"\"Save JSON data to a file.\"\"\"\n",
    "    with open(f\"data/api_response/{filename}\", \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"{filename} file saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map report methods or endpoint keys to filenames\n",
    "report_mappings = {\n",
    "    \"employees\": lambda: client.create_employees_report(),\n",
    "    \"account_info_fields\": lambda: client.get(endpoint_key=\"account_info_fields\"),\n",
    "    \"time_off_types\": lambda: client.get(endpoint_key=\"time_off_types\"),\n",
    "    \"time_off_policies\": lambda: client.get(endpoint_key=\"time_off_policies\"),\n",
    "    \"time_off_requests\": lambda: client.get_time_off_request(),\n",
    "    \"time_off_whos_out\": lambda: client.get(endpoint_key=\"time_off_whos_out\"),\n",
    "    \"applicant_tracking_jobs\": lambda: client.get(endpoint_key=\"applicant_tracking_jobs\"),\n",
    "    \"applicant_tracking_applications\": lambda: client.get_appl_trk_apps(),\n",
    "    \"applicant_application_details\": lambda: client.get_application_details(),\n",
    "}\n",
    "\n",
    "# Fetch and save reports dynamically\n",
    "for report_key, filename_or_callable in report_mappings.items():\n",
    "    if callable(filename_or_callable):\n",
    "        data = filename_or_callable()\n",
    "        filename = f\"{report_key}.json\"\n",
    "    else:\n",
    "        data = client.get(endpoint_key=report_key)\n",
    "        filename = filename_or_callable\n",
    "    save_json_data(data, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Save data in delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaFileManager:\n",
    "    \"\"\"Handles the creation, transformation, and storage of data in Delta format.\"\"\"\n",
    "\n",
    "    def __init__(self, app_name: str = \"MyApp\", encryption_key: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the Spark session with Delta Lake configurations and optionally sets the encryption key.\n",
    "\n",
    "        Args:\n",
    "            app_name (str): Name of the Spark application.\n",
    "            encryption_key (str): Encryption key for AES encryption (default: None).\n",
    "        \"\"\"\n",
    "        self.spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(app_name)\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        self.encryption_key = encryption_key\n",
    "\n",
    "    def convert_to_json_string(self, data: Union[List[dict], dict]) -> list:\n",
    "        \"\"\"\n",
    "        Converts a dictionary or a list of dictionaries into a list of JSON strings.\n",
    "\n",
    "        Args:\n",
    "            data (Union[List[dict], dict]): The input data, either a list of dictionaries\n",
    "                or a single dictionary.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of JSON strings.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input data is neither a list of dictionaries nor a dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            # Convert a single dictionary to a JSON string\n",
    "            return [json.dumps(data, default=str)]\n",
    "        elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
    "            # Convert each dictionary in the list to a JSON string\n",
    "            return [json.dumps(record, default=str) for record in data]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Input data must be a dictionary or a list of dictionaries.\"\n",
    "            )\n",
    "\n",
    "    def create_schemaless_df(self, json_strings: list) -> DataFrame:\n",
    "        \"\"\"Converts a list of JSON strings into a schemaless DataFrame.\"\"\"\n",
    "        return self.spark.createDataFrame(json_strings, StringType())\n",
    "\n",
    "    def add_processed_dt(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Adds a processed timestamp column to a DataFrame.\"\"\"\n",
    "        return df.withColumn(\"processed_at\", F.current_timestamp())\n",
    "\n",
    "    def encrypt_columns(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        columns: list,\n",
    "        encryption_mode: str = \"ECB\",\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Encrypts the specified columns in the DataFrame using AES encryption and Base64 encoding.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            columns (list): List of column names to encrypt.\n",
    "            encryption_mode (str): The encryption mode for AES (default: \"ECB\").\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The DataFrame with encrypted columns.\n",
    "        \"\"\"\n",
    "        if not self.encryption_key:\n",
    "            raise ValueError(\n",
    "                \"Encryption key is not set. Please provide an encryption key.\"\n",
    "            )\n",
    "\n",
    "        for col_name in columns:\n",
    "            encrypted_col = F.expr(\n",
    "                f\"aes_encrypt({col_name}, '{self.encryption_key}', '{encryption_mode}')\"\n",
    "            )\n",
    "            base64_encoded_col = F.base64(encrypted_col)\n",
    "            df = df.withColumn(col_name, base64_encoded_col)\n",
    "        return df\n",
    "\n",
    "    def save_to_delta(\n",
    "        self, df: DataFrame, path: str, repartition: int = 1, mode: str = \"append\"\n",
    "    ):\n",
    "        \"\"\"Saves a DataFrame to a Delta table.\"\"\"\n",
    "        df.repartition(repartition).write.format(\"delta\").mode(mode).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the manager\n",
    "manager = DeltaFileManager(encryption_key=encryption_key)\n",
    "\n",
    "# Directory containing JSON files\n",
    "data_folder = \"data/api_response/\"\n",
    "\n",
    "# Iterate over all JSON files in the folder\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        # Read the JSON file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        # Convert the JSON data to JSON strings\n",
    "        json_strings = manager.convert_to_json_string(data)\n",
    "        # Create schemaless DataFrame\n",
    "        df = manager.create_schemaless_df(json_strings)\n",
    "        # Add a processed timestamp column\n",
    "        df = manager.add_processed_dt(df)\n",
    "        # Encrypt data (if required)\n",
    "        df = manager.encrypt_columns(df, columns=[\"value\"])\n",
    "        # Generate a Delta table path based on the filename\n",
    "        table_name = filename.replace(\".json\", \"\")\n",
    "        delta_table_path = f\"data/delta_tables/{table_name}\"\n",
    "        # Save the DataFrame to Delta table\n",
    "        manager.save_to_delta(df, delta_table_path, mode=\"overwrite\")\n",
    "\n",
    "        print(f\"Processed and saved: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decrypt value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_columns(\n",
    "    df: DataFrame,\n",
    "    columns: list,\n",
    "    encryption_key: str = encryption_key,\n",
    "    encryption_mode: str = \"ECB\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Decrypts the specified columns in the DataFrame using AES decryption and Base64 decoding.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to decrypt.\n",
    "        encryption_key (str): The decryption key for AES decryption.\n",
    "        encryption_mode (str): The decryption mode for AES (default: \"ECB\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with decrypted columns.\n",
    "    \"\"\"\n",
    "    for col_name in columns:\n",
    "        decrypted_col = F.expr(\n",
    "            f\"aes_decrypt(unbase64({col_name}), '{encryption_key}', '{encryption_mode}')\"\n",
    "        ).cast(\"string\")\n",
    "        df = df.withColumn(col_name, decrypted_col)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check decryption\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"app_name\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "delta_table_path = \"/Users/emif/Documents/payclip/data-tools/spark-warehouse/mutt_data.db/employee_data\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = decrypt_columns(df, columns=[\"value\"])\n",
    "df.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
