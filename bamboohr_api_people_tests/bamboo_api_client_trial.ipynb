{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from typing import List, Union\n",
    "from dotenv import load_dotenv\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql.types import StringType\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import DataFrame, SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "encryption_key = os.getenv(\"ENCRYPTION_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BambooHR API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPLOYEES_REPORT_PAYLOAD = {\n",
    "    \"title\": \"custom-employees\",\n",
    "    \"fields\": [\n",
    "        \"displayName\",\n",
    "        \"firstName\",\n",
    "        \"lastName\",\n",
    "        \"preferredName\",\n",
    "        \"dateOfBirth\",\n",
    "        \"maritalStatus\",\n",
    "        \"ssn\",\n",
    "        \"gender\",\n",
    "        \"pronouns\",\n",
    "        \"country\",\n",
    "        \"city\",\n",
    "        \"zipcode\",\n",
    "        \"address1\",\n",
    "        \"address2\",\n",
    "        \"employee_access\",\n",
    "        \"customShirtsize\",\n",
    "        \"customHobbies\",\n",
    "        \"allergies\",\n",
    "        \"dietaryRestrictions\",\n",
    "        \"jobTitle\",\n",
    "        \"hireDate\",\n",
    "        \"originalHireDate\",\n",
    "        \"employeeStatusDate\",\n",
    "        \"employmentStatus\",\n",
    "        \"employmentHistoryStatus\",  # FTE\n",
    "        \"terminationDate\",\n",
    "        \"location\",\n",
    "        \"workPhone\",\n",
    "        \"mobilePhone\",\n",
    "        \"workEmail\",\n",
    "        \"department\",\n",
    "        \"division\",\n",
    "        \"workPhoneExtension\",\n",
    "        \"supervisor\",\n",
    "        \"supervisorEid\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BambooHRClient:\n",
    "    \"\"\"A client for the BambooHR API.\n",
    "    This client handles authentication, session management, and API requests\n",
    "    to the BambooHR API or a placeholder API for demonstration purposes.\n",
    "    \"\"\"\n",
    "\n",
    "    DOMAIN = \"muttclip\"\n",
    "    BASE_URL = f\"https://api.bamboohr.com/api/gateway.php/{DOMAIN}/v1\"\n",
    "    ENDPOINTS = {\n",
    "        \"company_information\": \"/company_information\",\n",
    "        # Headcount\n",
    "        \"employees\": \"/reports/custom\",\n",
    "        \"employees_changed\": \"/employees/changed\",\n",
    "        \"account_info_fields\": \"/meta/fields\",\n",
    "        \"account_info_tab_fields\": \"/meta/tables\",\n",
    "        \"account_list_fields\": \"/meta/lists\",\n",
    "        \"account_list_users\": \"/meta/users\",\n",
    "        # Time Off\n",
    "        \"time_off_types\": \"/meta/time_off/types\",\n",
    "        \"time_off_policies\": \"/meta/time_off/policies\",\n",
    "        \"time_off_requests\": \"/time_off/requests\",\n",
    "        \"time_off_whos_out\": \"/time_off/whos_out\",\n",
    "        # Hiring\n",
    "        \"applicant_tracking_jobs\": \"/applicant_tracking/jobs\",\n",
    "        \"applicant_tracking_applications\": \"/applicant_tracking/applications\",\n",
    "        \"applicant_application_details\": \"/applicant_tracking/applications/{appId}\",\n",
    "        ##\n",
    "        \"custom_report_test\": \"/reports/101\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, api_key: str, days_offset: int = 1):\n",
    "        \"\"\"Initialize the BambooHRClient with an API key.\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.days_offset = days_offset\n",
    "        self.base_url = self.BASE_URL\n",
    "        self.session = self._create_session()\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": self._encode_auth_header(api_key),\n",
    "        }\n",
    "\n",
    "    def _encode_auth_header(self, api_key: str) -> str:\n",
    "        \"\"\"Encode the API key in Base64 for the Authorization header.\"\"\"\n",
    "        auth_string = f\"{api_key}:x\"\n",
    "        encoded_bytes = base64.b64encode(auth_string.encode(\"utf-8\"))\n",
    "        return f\"Basic {encoded_bytes.decode('utf-8')}\"\n",
    "\n",
    "    def _build_query_string(self, params: dict) -> str:\n",
    "        \"\"\"Helper method to construct a query string from a dictionary.\"\"\"\n",
    "        return \"&\".join(f\"{k}={v}\" for k, v in params.items())\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create a session with a retry strategy for handling transient errors.\"\"\"\n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=2,\n",
    "            status_forcelist=[429, 503],\n",
    "            allowed_methods=[\"GET\", \"POST\"],\n",
    "        )\n",
    "\n",
    "        # Create an adapter with the retry strategy\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "        # Create a session and mount the adapter\n",
    "        session = requests.Session()\n",
    "        session.mount(\"https://\", adapter)\n",
    "\n",
    "        return session\n",
    "\n",
    "    def get(\n",
    "        self, endpoint_key: str = None, endpoint_path: str = None, params: dict = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch data from a specified endpoint using the endpoint key or a direct endpoint path.\n",
    "\n",
    "        Args:\n",
    "            endpoint_key (str, optional): The key for the desired endpoint (e.g., \"posts\"). Defaults to None.\n",
    "            endpoint_path (str, optional): The direct path for the endpoint (e.g., \"/employees/123\"). Defaults to None.\n",
    "            params (dict, optional): Query parameters to include in the request.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If neither `endpoint_key` nor `endpoint_path` is provided.\n",
    "            requests.exceptions.RequestException: If the request fails.\n",
    "        \"\"\"\n",
    "        if endpoint_key:\n",
    "            endpoint = self.ENDPOINTS.get(endpoint_key)\n",
    "            if not endpoint:\n",
    "                raise ValueError(f\"Invalid endpoint key: {endpoint_key}\")\n",
    "        elif endpoint_path:\n",
    "            endpoint = endpoint_path\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"You must provide either an endpoint_key or an endpoint_path.\"\n",
    "            )\n",
    "\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = self.session.get(url, headers=self.headers, params=params)\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with status {response.status_code}: {e}\")\n",
    "            raise\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    def get_employees_changed(self, change_type: str = None) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch the list of employees changed since a given number of days ago.\n",
    "\n",
    "        Args:\n",
    "            change_type (str, optional): Type of change to filter for (\"inserted\", \"updated\", \"deleted\").\n",
    "                If not provided, all change types will be included.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API, containing the list of employees\n",
    "                who have changed since the specified timestamp.\n",
    "        \"\"\"\n",
    "        since = datetime.now(timezone.utc) - timedelta(days=self.days_offset)\n",
    "        since = since.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        params = {\"since\": since}\n",
    "        if change_type:\n",
    "            params[\"type\"] = change_type\n",
    "\n",
    "        return self.get(\"employees_changed\", params=params)\n",
    "\n",
    "    def get_time_off_request(self) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch time-off requests within a specified date range.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API, containing the list of time-off\n",
    "                requests within the specified date range.\n",
    "        \"\"\"\n",
    "        start_dt = datetime.now(timezone.utc) - timedelta(days=self.days_offset)\n",
    "        end_dt = datetime.now(timezone.utc)\n",
    "        start_dt = start_dt.strftime(\"%Y-%m-%d\")\n",
    "        end_dt = end_dt.strftime(\"%Y-%m-%d\")\n",
    "        params = {\n",
    "            \"start\": start_dt,\n",
    "            \"end\": end_dt,\n",
    "        }\n",
    "\n",
    "        return self.get(\"time_off_requests\", params=params)\n",
    "\n",
    "    def get_appl_trk_apps(self) -> dict:\n",
    "        \"\"\"\n",
    "        Fetches applicant tracking applications updated since a specified number of days ago.\n",
    "\n",
    "        This method generates the `newSince` query parameter using the current UTC time\n",
    "        minus the specified number of days. It then sends a GET request to the\n",
    "        `applicant_tracking_applications` endpoint with the constructed parameter.\n",
    "\n",
    "        Returns:\n",
    "            dict: The API response containing applicant tracking applications.\n",
    "        \"\"\"\n",
    "        new_since = datetime.now(timezone.utc) - timedelta(days=self.days_offset)\n",
    "        new_since = new_since.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        params = {\"newSince\": new_since}\n",
    "\n",
    "        return self.get(\"applicant_tracking_applications\", params=params)\n",
    "\n",
    "    def get_application_details(self) -> list:\n",
    "        \"\"\"\n",
    "        Fetches detailed information for each application in the applicant tracking system.\n",
    "\n",
    "        This method first retrieves a list of applications using the `get_appl_trk_apps` method,\n",
    "        extracts their IDs, and then fetches detailed information for each application by calling\n",
    "        the BambooHR API endpoint `/applicant_tracking/applications/{applicationId}`.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, where each dictionary contains detailed data for a single application.\n",
    "\n",
    "        Notes:\n",
    "            - The method uses the `get_appl_trk_apps` method to fetch the initial list of applications.\n",
    "            - The `get` method is used with `endpoint_path` to fetch details for each `applicationId`.\n",
    "\n",
    "        Error Handling:\n",
    "            - If an exception occurs during the detailed fetch for a specific application,\n",
    "            the method logs the error and continues processing the remaining application IDs.\n",
    "        \"\"\"\n",
    "        # Fetch the initial applications report and get the application IDs\n",
    "        applications_report = self.get_appl_trk_apps()\n",
    "        application_ids = [\n",
    "            app[\"id\"] for app in applications_report.get(\"applications\", [])\n",
    "        ]\n",
    "        # Fetch detailed data for each application\n",
    "        detailed_applications = []\n",
    "        for app_id in application_ids:\n",
    "            try:\n",
    "                details = self.get(\n",
    "                    endpoint_path=f\"/applicant_tracking/applications/{app_id}\"\n",
    "                )\n",
    "                detailed_applications.append(details)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch details for application ID {app_id}: {e}\")\n",
    "\n",
    "        return detailed_applications\n",
    "\n",
    "    def post(\n",
    "        self, endpoint_key: str, data: dict = None, query_params: dict = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Send data to a specified endpoint using the POST method.\n",
    "\n",
    "        Args:\n",
    "            endpoint_key (str): The key for the desired endpoint.\n",
    "            data (dict, optional): The JSON payload to send in the POST request.\n",
    "            query_params (dict, optional): Query parameters to include in the URL.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an invalid endpoint key is provided.\n",
    "            requests.exceptions.RequestException: If the request fails.\n",
    "        \"\"\"\n",
    "        endpoint = self.ENDPOINTS.get(endpoint_key)\n",
    "        if not endpoint:\n",
    "            raise ValueError(f\"Invalid endpoint key: {endpoint_key}\")\n",
    "\n",
    "        if query_params:\n",
    "            query_string = self._build_query_string(query_params)\n",
    "            url = f\"{self.base_url}{endpoint}?{query_string}\"\n",
    "        else:\n",
    "            url = f\"{self.base_url}{endpoint}\"\n",
    "\n",
    "        response = self.session.post(url, headers=self.headers, json=data)\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with status {response.status_code}: {e}\")\n",
    "            raise\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    def create_employees_report(self) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a custom report of employees and all their relevant data.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API with the generated report.\n",
    "        \"\"\"\n",
    "        payload = EMPLOYEES_REPORT_PAYLOAD\n",
    "        q_params = {\"format\": \"json\", \"onlyCurrent\": \"true\"}\n",
    "\n",
    "        return self.post(\"employees\", data=payload, query_params=q_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client = BambooHRClient(api_key=api_key, days_offset=30)\n",
    "\n",
    "# Create a data directory to store the JSON outputs\n",
    "os.makedirs(\"data/api_response\", exist_ok=True)\n",
    "\n",
    "\n",
    "def save_json_data(data: dict, filename: str) -> None:\n",
    "    \"\"\"Save JSON data to a file.\"\"\"\n",
    "    with open(f\"data/api_response/{filename}\", \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"{filename} file saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map report methods or endpoint keys to filenames\n",
    "report_mappings = {\n",
    "    \"company_information\": \"company_information.json\",\n",
    "    \"employees\": lambda: client.create_employees_report(),\n",
    "    \"employees_changed\": lambda: client.get_employees_changed(),\n",
    "    \"account_info_fields\": \"account_info_fields.json\",\n",
    "    \"account_info_tab_fields\": \"account_info_tab_fields.json\",\n",
    "    \"account_list_fields\": \"account_list_fields.json\",\n",
    "    \"account_list_users\": \"account_list_users.json\",\n",
    "    \"time_off_types\": \"time_off_types.json\",\n",
    "    \"time_off_policies\": \"time_off_policies.json\",\n",
    "    \"time_off_requests\": lambda: client.get_time_off_request(),\n",
    "    \"time_off_whos_out\": \"time_off_whos_out.json\",\n",
    "    \"applicant_tracking_jobs\": \"applicant_tracking_jobs.json\",\n",
    "    \"applicant_tracking_applications\": lambda: client.get_appl_trk_apps(),\n",
    "    \"applicant_application_details\": lambda: client.get_application_details(),\n",
    "    # \"custom_report_test\": \"custom_report_test.json\",\n",
    "}\n",
    "\n",
    "# Fetch and save reports dynamically\n",
    "for report_key, filename_or_callable in report_mappings.items():\n",
    "    if callable(filename_or_callable):\n",
    "        data = filename_or_callable()\n",
    "        filename = f\"{report_key}.json\"\n",
    "    else:\n",
    "        data = client.get(endpoint_key=report_key)\n",
    "        filename = filename_or_callable\n",
    "    save_json_data(data, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Save data in delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaFileManager:\n",
    "    \"\"\"Handles the creation, transformation, and storage of data in Delta format.\"\"\"\n",
    "\n",
    "    def __init__(self, app_name: str = \"MyApp\", encryption_key: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the Spark session with Delta Lake configurations and optionally sets the encryption key.\n",
    "\n",
    "        Args:\n",
    "            app_name (str): Name of the Spark application.\n",
    "            encryption_key (str): Encryption key for AES encryption (default: None).\n",
    "        \"\"\"\n",
    "        builder = (\n",
    "            SparkSession.builder.appName(app_name)\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\n",
    "                \"spark.sql.catalog.spark_catalog\",\n",
    "                \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "            )\n",
    "        )\n",
    "        self.spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "        self.encryption_key = encryption_key\n",
    "\n",
    "    def convert_to_json_string(self, data: Union[List[dict], dict]) -> list:\n",
    "        \"\"\"\n",
    "        Converts a dictionary or a list of dictionaries into a list of JSON strings.\n",
    "\n",
    "        Args:\n",
    "            data (Union[List[dict], dict]): The input data, either a list of dictionaries\n",
    "                or a single dictionary.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of JSON strings.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input data is neither a list of dictionaries nor a dictionary.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            # Convert a single dictionary to a JSON string\n",
    "            return [json.dumps(data, default=str)]\n",
    "        elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
    "            # Convert each dictionary in the list to a JSON string\n",
    "            return [json.dumps(record, default=str) for record in data]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Input data must be a dictionary or a list of dictionaries.\"\n",
    "            )\n",
    "\n",
    "    def create_schemaless_df(self, json_strings: list) -> DataFrame:\n",
    "        \"\"\"Converts a list of JSON strings into a schemaless DataFrame.\"\"\"\n",
    "        return self.spark.createDataFrame(json_strings, StringType())\n",
    "\n",
    "    def add_processed_dt(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Adds a processed timestamp column to a DataFrame.\"\"\"\n",
    "        return df.withColumn(\"processed_at\", F.current_timestamp())\n",
    "\n",
    "    def encrypt_columns(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        columns: list,\n",
    "        encryption_mode: str = \"ECB\",\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Encrypts the specified columns in the DataFrame using AES encryption and Base64 encoding.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            columns (list): List of column names to encrypt.\n",
    "            encryption_mode (str): The encryption mode for AES (default: \"ECB\").\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The DataFrame with encrypted columns.\n",
    "        \"\"\"\n",
    "        if not self.encryption_key:\n",
    "            raise ValueError(\n",
    "                \"Encryption key is not set. Please provide an encryption key.\"\n",
    "            )\n",
    "\n",
    "        for col_name in columns:\n",
    "            encrypted_col = F.expr(\n",
    "                f\"aes_encrypt({col_name}, '{self.encryption_key}', '{encryption_mode}')\"\n",
    "            )\n",
    "            base64_encoded_col = F.base64(encrypted_col)\n",
    "            df = df.withColumn(col_name, base64_encoded_col)\n",
    "        return df\n",
    "\n",
    "    def save_to_delta(\n",
    "        self, df: DataFrame, path: str, repartition: int = 1, mode: str = \"append\"\n",
    "    ):\n",
    "        \"\"\"Saves a DataFrame to a Delta table.\"\"\"\n",
    "        df.repartition(repartition).write.format(\"delta\").mode(mode).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the manager\n",
    "manager = DeltaFileManager(encryption_key=encryption_key)\n",
    "\n",
    "# Directory containing JSON files\n",
    "data_folder = \"data/api_response/\"\n",
    "\n",
    "# Iterate over all JSON files in the folder\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        # Read the JSON file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        # Convert the JSON data to JSON strings\n",
    "        json_strings = manager.convert_to_json_string(data)\n",
    "        # Create schemaless DataFrame\n",
    "        df = manager.create_schemaless_df(json_strings)\n",
    "        # Add a processed timestamp column\n",
    "        df = manager.add_processed_dt(df)\n",
    "        # Encrypt data (if required)\n",
    "        df = manager.encrypt_columns(df, columns=[\"value\"])\n",
    "        # Generate a Delta table path based on the filename\n",
    "        table_name = filename.replace(\".json\", \"\")\n",
    "        delta_table_path = f\"data/delta_tables/{table_name}\"\n",
    "        # Save the DataFrame to Delta table\n",
    "        manager.save_to_delta(df, delta_table_path, mode=\"overwrite\")\n",
    "\n",
    "        print(f\"Processed and saved: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decrypt value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_columns(\n",
    "    df: DataFrame,\n",
    "    columns: list,\n",
    "    encryption_key: str = encryption_key,\n",
    "    encryption_mode: str = \"ECB\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Decrypts the specified columns in the DataFrame using AES decryption and Base64 decoding.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to decrypt.\n",
    "        encryption_key (str): The decryption key for AES decryption.\n",
    "        encryption_mode (str): The decryption mode for AES (default: \"ECB\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with decrypted columns.\n",
    "    \"\"\"\n",
    "    for col_name in columns:\n",
    "        decrypted_col = F.expr(\n",
    "            f\"aes_decrypt(unbase64({col_name}), '{encryption_key}', '{encryption_mode}')\"\n",
    "        ).cast(\"string\")\n",
    "        df = df.withColumn(col_name, decrypted_col)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check decryption\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"app_name\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "delta_table_path = \"data/delta_tables/employees\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df = decrypt_columns(df, columns=[\"value\"])\n",
    "df.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
