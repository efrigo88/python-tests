{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql.types import StringType\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import DataFrame, SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "encryption_key = os.getenv(\"ENCRYPTION_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BambooHR API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BambooHRClient:\n",
    "    \"\"\"A client for the BambooHR API.\n",
    "    This client handles authentication, session management, and API requests\n",
    "    to the BambooHR API or a placeholder API for demonstration purposes.\n",
    "    \"\"\"\n",
    "\n",
    "    DOMAIN = \"muttclip\"\n",
    "    BASE_URL = f\"https://api.bamboohr.com/api/gateway.php/{DOMAIN}/v1\"\n",
    "    ENDPOINTS = {\n",
    "        \"employees\": \"/employees/directory\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"Initialize the BambooHRClient with an API key.\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.base_url = self.BASE_URL\n",
    "        self.session = self._create_session()\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": self._encode_auth_header(api_key),\n",
    "        }\n",
    "\n",
    "    def _encode_auth_header(self, api_key: str) -> str:\n",
    "        \"\"\"Encode the API key in Base64 for the Authorization header.\"\"\"\n",
    "        auth_string = f\"{api_key}:x\"\n",
    "        encoded_bytes = base64.b64encode(auth_string.encode(\"utf-8\"))\n",
    "        return f\"Basic {encoded_bytes.decode('utf-8')}\"\n",
    "    \n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create a session with a retry strategy for handling transient errors.\"\"\"\n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=2,\n",
    "            status_forcelist=[429, 503],\n",
    "            allowed_methods=[\"GET\"],\n",
    "        )\n",
    "\n",
    "        # Create an adapter with the retry strategy\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "        # Create a session and mount the adapter\n",
    "        session = requests.Session()\n",
    "        session.mount(\"https://\", adapter)\n",
    "\n",
    "        return session\n",
    "\n",
    "    def get(self, endpoint_key: str, params: dict = None) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch data from a specified endpoint using the endpoint key.\n",
    "\n",
    "        Args:\n",
    "            endpoint_key (str): The key for the desired endpoint (e.g., \"posts\").\n",
    "            params (dict, optional): Query parameters to include in the request.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON response from the API.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an invalid endpoint key is provided.\n",
    "            requests.exceptions.RequestException: If the request fails.\n",
    "\n",
    "        Example:\n",
    "            >>> client = BambooHRClient(api_key=\"my_api_key\", company_domain=\"typicode\")\n",
    "            >>> posts = client.get(\"posts\")\n",
    "            >>> print(posts)\n",
    "        \"\"\"\n",
    "        endpoint = self.ENDPOINTS.get(endpoint_key)\n",
    "        if not endpoint:\n",
    "            raise ValueError(f\"Invalid endpoint key: {endpoint_key}\")\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = self.session.get(url, headers=self.headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client = BambooHRClient(api_key=api_key)\n",
    "\n",
    "employees = client.get(\"employees\")\n",
    "\n",
    "print(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees.json file saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Create a data directory and save the json outputs to check responses.\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "def save_json_data(data: dict, filename: str) -> None:\n",
    "    with open(f\"data/{filename}\", \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"{filename} file saved successfully\")\n",
    "\n",
    "for data, data_name in zip([employees,], [\"employees\",]):\n",
    "    save_json_data(data, f\"{data_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Save data in delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeltaFileManager:\n",
    "#     \"\"\"Handles the creation, transformation, and storage of data in Delta format.\"\"\"\n",
    "\n",
    "#     def __init__(self, app_name: str = \"MyApp\", encryption_key: str = None):\n",
    "#         \"\"\"\n",
    "#         Initializes the Spark session with Delta Lake configurations and optionally sets the encryption key.\n",
    "\n",
    "#         Args:\n",
    "#             app_name (str): Name of the Spark application.\n",
    "#             encryption_key (str): Encryption key for AES encryption (default: None).\n",
    "#         \"\"\"\n",
    "#         builder = (\n",
    "#             SparkSession.builder.appName(app_name)\n",
    "#             .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "#             .config(\n",
    "#                 \"spark.sql.catalog.spark_catalog\",\n",
    "#                 \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "#             )\n",
    "#         )\n",
    "#         self.spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "#         self.encryption_key = encryption_key\n",
    "\n",
    "#     def convert_to_json_string(self, data: List[dict]) -> list:\n",
    "#         \"\"\"\n",
    "#         Converts a list of dictionaries into a list of JSON strings.\n",
    "\n",
    "#         This method serializes each dictionary in the input list into a JSON-formatted\n",
    "#         string. The `default=str` parameter ensures that non-serializable types\n",
    "#         (e.g., datetime objects) are converted to strings during serialization.\n",
    "#         \"\"\"\n",
    "#         return [json.dumps(record, default=str) for record in data]\n",
    "\n",
    "#     def create_schemaless_df(self, json_strings: list) -> DataFrame:\n",
    "#         \"\"\"Converts a list of JSON strings into a schemaless DataFrame.\"\"\"\n",
    "#         return self.spark.createDataFrame(json_strings, StringType())\n",
    "\n",
    "#     def add_processed_dt(self, df: DataFrame) -> DataFrame:\n",
    "#         \"\"\"Adds a processed timestamp column to a DataFrame.\"\"\"\n",
    "#         return df.withColumn(\"processed_at\", F.current_timestamp())\n",
    "\n",
    "#     def encrypt_columns(\n",
    "#         self,\n",
    "#         df: DataFrame,\n",
    "#         columns: list,\n",
    "#         encryption_mode: str = \"ECB\",\n",
    "#     ) -> DataFrame:\n",
    "#         \"\"\"\n",
    "#         Encrypts the specified columns in the DataFrame using AES encryption and Base64 encoding.\n",
    "\n",
    "#         Args:\n",
    "#             df (DataFrame): The input DataFrame.\n",
    "#             columns (list): List of column names to encrypt.\n",
    "#             encryption_mode (str): The encryption mode for AES (default: \"ECB\").\n",
    "\n",
    "#         Returns:\n",
    "#             DataFrame: The DataFrame with encrypted columns.\n",
    "#         \"\"\"\n",
    "#         if not self.encryption_key:\n",
    "#             raise ValueError(\"Encryption key is not set. Please provide an encryption key.\")\n",
    "\n",
    "#         for col_name in columns:\n",
    "#             encrypted_col = F.expr(f\"aes_encrypt({col_name}, '{self.encryption_key}', '{encryption_mode}')\")\n",
    "#             base64_encoded_col = F.base64(encrypted_col)\n",
    "#             df = df.withColumn(col_name, base64_encoded_col)\n",
    "#         return df\n",
    "\n",
    "#     def save_to_delta(\n",
    "#         self, df: DataFrame, path: str, repartition: int = 1, mode: str = \"append\"\n",
    "#     ):\n",
    "#         \"\"\"Saves a DataFrame to a Delta table.\"\"\"\n",
    "#         df.repartition(repartition).write.format(\"delta\").mode(mode).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the manager\n",
    "# manager = DeltaFileManager(encryption_key=encryption_key)\n",
    "\n",
    "# # Prepare input files into json_strings\n",
    "# posts_string = manager.convert_to_json_string(posts)\n",
    "# albums_string = manager.convert_to_json_string(albums)\n",
    "# users_string = manager.convert_to_json_string(users)\n",
    "\n",
    "# # Create schemaless DataFrames\n",
    "# posts_df = manager.create_schemaless_df(posts_string)\n",
    "# albums_df = manager.create_schemaless_df(albums_string)\n",
    "# users_df = manager.create_schemaless_df(users_string)\n",
    "\n",
    "# # Add a timestamp column\n",
    "# posts_df = manager.add_processed_dt(posts_df)\n",
    "# albums_df = manager.add_processed_dt(albums_df)\n",
    "# users_df = manager.add_processed_dt(users_df)\n",
    "\n",
    "# # Encrypt data\n",
    "# posts_df = manager.encrypt_columns(posts_df, columns=[\"value\"])\n",
    "# albums_df = manager.encrypt_columns(albums_df, columns=[\"value\"])\n",
    "# users_df = manager.encrypt_columns(users_df, columns=[\"value\"])\n",
    "\n",
    "# # Save to Delta tables\n",
    "# manager.save_to_delta(posts_df, \"data/delta_tables/posts\", mode=\"overwrite\")\n",
    "# manager.save_to_delta(albums_df, \"data/delta_tables/albums\", mode=\"overwrite\")\n",
    "# manager.save_to_delta(users_df, \"data/delta_tables/users\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Schemas\n",
    "# posts_df.printSchema()\n",
    "# albums_df.printSchema()\n",
    "# users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display results\n",
    "# posts_df.show(1, truncate=False)\n",
    "# albums_df.show(1, truncate=False)\n",
    "# users_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decrypt value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decrypt_columns(\n",
    "#         df: DataFrame, \n",
    "#         columns: list, \n",
    "#         encryption_key: str = encryption_key, \n",
    "#         encryption_mode: str = \"ECB\") -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Decrypts the specified columns in the DataFrame using AES decryption and Base64 decoding.\n",
    "\n",
    "#     Args:\n",
    "#         df (DataFrame): The input DataFrame.\n",
    "#         columns (list): List of column names to decrypt.\n",
    "#         encryption_key (str): The decryption key for AES decryption.\n",
    "#         encryption_mode (str): The decryption mode for AES (default: \"ECB\").\n",
    "\n",
    "#     Returns:\n",
    "#         DataFrame: The DataFrame with decrypted columns.\n",
    "#     \"\"\"\n",
    "#     for col_name in columns:\n",
    "#         decrypted_col = F.expr(\n",
    "#             f\"aes_decrypt(unbase64({col_name}), '{encryption_key}', '{encryption_mode}')\"\n",
    "#         ).cast(\"string\")\n",
    "#         df = df.withColumn(col_name, decrypted_col)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encrypt data\n",
    "# posts_df = decrypt_columns(posts_df, columns=[\"value\"])\n",
    "# albums_df = decrypt_columns(albums_df, columns=[\"value\"])\n",
    "# users_df = decrypt_columns(users_df, columns=[\"value\"])\n",
    "\n",
    "# # Display results\n",
    "# posts_df.show(1, truncate=False)\n",
    "# albums_df.show(1, truncate=False)\n",
    "# users_df.show(1, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
